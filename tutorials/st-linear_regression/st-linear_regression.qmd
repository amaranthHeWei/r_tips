---
title: "Build a linear regression model"
author: "Erika Duan"
date: last-modified
format: 
    gfm:
        html-math-method: webtex   
        toc: true
        toc-depth: 2   
        toc-title: Contents  
execute:   
  echo: true     
  output: false   
  warning: false  
---

```{r setup, include=FALSE}
# Set up global environment ----------------------------------------------------
knitr::opts_chunk$set(echo=TRUE, results="hide", message=FALSE, warning=FALSE)  
```

```{r}
# Load required R packages -----------------------------------------------------
if (!require("pacman")) install.packages("pacman")
pacman::p_load(here,
               tidymodels, 
               broom, # Provides model outputs as a tidy table
               modelsummary) # Plots model properties  
```

# Why linear regression?    

Linear regression is usually the first statistical model that people learn about. Although it has a reputation for being a simple method, linear regression is still used for different purposes.   

As listed in [Regression and Other Stories](https://avehtari.github.io/ROS-Examples/) by Gelman et al, linear regression can be used to:   

+ Predict or forecast outcomes without aiming to infer causality.   
+ Generate a linear explanation of the associations between independent variables (also known as features) and an outcome.    
+ Adjust outcomes from a sample to infer something about a population of interest.    
+ Estimate treatment effects by comparing outcomes between a treatment and control group in a randomised controlled trial.    

Linear regression models can be easily misused when purposes deviate from the ones described above. For example, people can mistake the associations produced by a linear regression model as being causal rather than just predictive. This is especially misleading [when some independent variables are predictive of each other as well as the outcome of interest](https://elevanth.org/blog/2021/06/15/regression-fire-and-dangerous-things-1-3/).   


# Build a linear regression model 

Let's first build a linear regression model and see what results it produces. We will then learn about the mathematical properties, interpretation and assumptions of our model.      

We will provide ourselves with a safety check, by secretly knowing the precise relationship between our independent variables and the outcome. This will obviously never happen in real life.   

Imagine that the amount of money a pet influencer earns per month is influenced by the following variables:    

+ A baseline monthly income   
+ Whether the pet is a dog or another animal species       
+ The number of photos their owner posts every month      
+ The number of videos their owner posts every month      

We also state that there are no confounds between these variables. This means that the value of one variable does not influence the value of another variable.   

```{mermaid}
flowchart LR  
  A(Baseline income) --> B(Monthly income)     
  C(Is dog) --> B 
  D(Photos per month) --> B
  E(Videos per month) --> B
  
  style B fill:#Fff9e3,stroke:#333
```

We can then simulate some income data to use for modelling.  

```{r}
# Simulate pet influencer income dataset ---------------------------------------
set.seed(111)
N <- 500 # Simulate 500 observations 

# Simulate whether pet is dog, cat or other from a multinomial distribution  
species <- rmultinom(N, 
                     size = 1,
                     prob = c(0.6, 0.3, 0.1))

# Convert species into dummy variables     
is_dog <- species[1, 1:N]
is_cat <- species[2, 1:N]

# Simulate number of photos per month from a poisson distribution 
photos <- rpois(N, lambda = 6)

# Simulate number of videos per month from a poisson distribution
videos <- rpois(N, lambda = 2)

# Simulate monthly income from a normal distribution 
# Mean monthly income has an intercept of 20 and is only determined by the 
# variables is_dog, photos and videos

income <- rnorm(N,
                mean = 20 + is_dog * 60 + photos * 6 + videos * 18,
                sd = 5)

# Ensure that income is a non-negative integer
income <- ifelse(income > 0, round(income, digits = 0), 0)

# Create dataset
data <- data.frame(
  is_dog,
  is_cat,
  photos,
  videos, 
  income
)
```

We will then split the simulated data into training and test data subsets with a 75% versus 25% split.      

```{r}
# Split data into training and test data sets using base R ---------------------
set.seed(111)

# Calculate 75% of the whole data set
train_size <- floor(0.75 * nrow(data))

# Randomly sample train_size number of rows and extract the row index
train_index <- sample(seq_len(nrow(data)), size = train_size) 

train <- data[train_index, ] # Subset by train_index row index
test <- data[-train_index, ] # Subset the remaining rows
```

We will then perform multiple linear regression modelling on the training data set and view the results.    

```{r}
#| output: true

# Perform multiple linear regression -------------------------------------------
# Models are fitted with the syntax lm(Y ~ X1 + X2 + ... + Xn) where Y is the
# outcome of interest and X1 ... Xn are distinct independent variables. 

mlr_model <- lm(
  train$income ~ # Y 
    train$is_dog + # X1 
    train$is_cat + # X2 
    train$photos + # X3 
    train$videos # X4 
)

# View the results of the fitted model using summary()
summary(mlr_model)
```

This looks very complicated! Let us interpret the key results above by learning about the mathematical structure of a linear regression model.   


# The mathemathical intuition   

The simplest linear regression model has the form $\hat {Y_i} = \beta_0 + \beta_1X_i$.   

Graphically, this is a line of best fit through the 2D Cartesian plane. To construct this model, we only need to estimate the two parameters of an unknown straight line:  

+ The y-intercept, which is what $\beta_0$ represents    
+ The slope, which is what $\beta_1$ represents     

We first assume that there is a **true model** which precisely predicts our outcome of interest $Y_i$ based on our independent variable of interest $X_i$. The true model has the form $Y_i = \beta_0 + \beta_1X_1 + \epsilon_i$, where $\epsilon_i$ represents error due to natural variation as objects do not behave like identical clones in the real world.    

![](../../figures/st-linear_regression-true_model_structure.gif)     

Because there is always error due to natural variation, we view each observation of $Y_i$ as being drawn from a normal distribution of many possible values. After making some assumptions about how $\epsilon_i$ behaves, we can claim that the **mean** of the probability distribution of $Y_i$ is $E(Y_i) = \beta_0 + \beta_1X_1$ where $E(Y_i)$ is the unknown straight line that we want to estimate.      

![](../../figures/st-linear_regression-y_normal_distribution.svg)

We want to use our training data set to find the best point estimates of $\beta_0$ and $\beta_1$. This means finding the line that travels closest through all the training data set observations. This line is our **best estimated model**, which has the form $\hat Y_i = b_0 + b_1X_i$. It has a y-intercept of $b_0$ and slope of $b_1$.   

![](../../figures/st-linear_regression-estimated_model_structure.gif) 

The model coefficients $b_0$ and $b_1$ are our best point estimates of the unknown $\beta_0$ and $\beta_1$ parameters. The point estimates for $b_0$ and $b_1$ are usually slightly different depending on which observations are present in the training data set.        

In this tutorial, we hypothesised that our multiple regression model had the form `lm(income ~ is_dog + is_cat + photos + videos)`. This means that we think that the mean monthly pet influencer income is the sum of:    

+ A potential baseline income (a non-zero $\beta_0$ value)   
+ An additional amount of money if the pet is a dog (a non-zero $\beta_1$ value)     
+ An additional amount of money if the pet is a cat (a non-zero $\beta_2$ value)   
+ An additional amount of money for each photo posted per month (a non-zero $\beta_3$ value)  
+ An additional amount of money for each video posted per month (a non-zero $\beta_4$ value)  

Mathematically, our best estimated model has the following structure, where the model coefficients $b_0, \cdots, b_4$ are the point estimates for $\beta_0, \cdots, \beta_4$ respectively.    

![](../../figures/st-linear_regression-tutorial_model_structure.svg)
The model coefficients $b_0, \cdots, b_4$ also explain exactly how our model predicts the mean monthly pet influencer income.   

![](../../figures/st-linear_regression-coefficients_explained_1.svg)  

![](../../figures/st-linear_regression-coefficients_explained_2.svg)  

Let us examine our `mlr_model` coefficients. We can output them into a tabular format using the `tidy()` function from the [`broom`](https://cran.r-project.org/web/packages/broom/vignettes/broom.html) package.      

```{r}
#| output: true

# Extract model coefficients and output as a tidy table ------------------------
mlr_model |> tidy()
```

The **point estimates** of the model coefficients are used to construct our best estimated model, which has the following form.   

![](../../figures/st-linear_regression-tutorial_best_estimated_model.svg)  

After examining the **point estimates**, **standard errors** and **p-values** of our model coefficients, we can also claim that the following associations exist, provided that our modelling assumptions are reasonable:   

+ A monthly baseline income of ~21.5 dollars exists. A pet influencer who posts 0 photos and videos where the pet is not a dog or a cat earns an average of ~21.5 dollars per month.            
+ If the pet is a dog, the monthly income additionally increases by ~58.9 dollars.   
+ There is no additional monthly income increase if the pet is a cat.     
+ For each additional photo posted, the monthly income increases by ~5.9 dollars.   
+ For each additional video posted, the monthly income increases by ~17.9 dollars.  

Inspecting the standard errors and p-values are important. The standard error helps to estimate the range of values that our true model parameter is likely to fall within. The p-value is used as a yardstick to conclude if our true model parameters are indeed non-zero.    

We can also extract and plot 95% confidence intervals for our model coefficients. The 95% confidence interval is a random interval that is expected to contain a parameter of interest 95% of the time. A narrow and non-zero confidence interval for $\beta_0, \cdots, \beta_4$ indicates a convincing association between an independent variable and the outcome.    

```{r}
#| output: true

# Output 95% confidence intervals for model coefficients -----------------------
# We can also output confidence intervals as a tidy table using broom::tidy()
# mlr_model |> tidy(conf.int = TRUE, conf.level = 0.95)

confint(mlr_model)
```

The plot below clearly illustrates that being a cat is the only independent variable that is not predictive of monthly pet influencer income.  

```{r}
#| output: true

# Plot 95% confidence intervals for model coefficients -------------------------
# Set p-value cut-off at p < 0.001 

coef_names <- c(
  "(Intercept)" = "Intercept",
  "train$is_dog" = "Is a dog",
  "train$is_cat" = "Is a cat",
  "train$photos" = "Photos",
  "train$videos" = "Videos"
)

modelplot(mlr_model,
          coef_map = coef_names) +
  geom_vline(xintercept = 0, 
             colour = "firebrick",
             linetype = "dotted") +
  aes(colour = ifelse(p.value < 0.001, "Significant", "Not significant")) +
  scale_colour_manual(values = c("grey", "black")) + 
  labs(title = "95% CI for linear regression model coefficients",
         x = "Coefficients",
       colour = NULL) +
    theme_classic() +
  theme(panel.grid.major.x = element_line(colour = "grey70",
                                          linetype = "dotted"))
```
  
Although model coefficients tell us how our model makes a prediction and which independent variables are predictive of the outcome, we cannot use them to evaluate whether our model is actually a good or poor one. To evaluate our model, we need to examine different metrics.  


# Evaluate a linear regression model   

A linear regression model outputs several metrics and plots which are useful for model evaluation. A summary of the key model evaluation metrics are below.   

**Note:** $p$ represents the number of independent variables in the model and $n$ represents the number of observations in the data set used to train the model.    

| Metric | Mathematical form | Description | 
|:-------|:------------|   
| Residual mean square (MSE) | $\frac{\sum_{i=1}^n (Y_i - \hat {Y_i})^2}{n-p}$ | A metric to evaluate the magnitude of the error between the true and estimated outcome. In linear regression, MSE is specifically the point estimator for $Var(\epsilon_i)$ assuming that $\epsilon_i$ is normally distributed. The MSE should be evaluated on the test data set. |       
| Residual standard error (RSE) | $\sqrt{\frac{\sum_{i=1}^n (Y_i - \hat {Y_i})^2}{n-p}}$ | Another metric to evaluate the magnitude of the error between the true and estimated outcome. RSE can be preferred over MSE as it has the same units as the outcome of interest $Y_i$. The RSE should be evaluated on the test data set. |      
| F statistic | $\frac{MSR}{MSE} \sim F(p-1, n-p)$ where $MSR = \frac{\sum_{i=1}^n (\hat{Y_i} - \bar Y)^2}{p-1}$ | A statistical test with a null hypothesis that all model coefficients are zero. This metric describes a statistical property of the trained model and is not really useful for model evaluation or interpretation. |    
| Multiple $r^2$ | $1 - \frac{SSE}{SSTO}$ where $ SSE = \sum_{i=1}^n (Y_i - \hat Y_i)^2$ and $SSTO = \sum_{i=1}^n (Y_i - \bar Y)^2$ | This metric calculates the proportion of total variation in the outcome of interest that is associated with the independent variables in the training model. When the model perfectly predicts all observations in the training data set, $r^2 = 1$. This metric is a property of the trained model and should not be used to identify the best model. |        
| Adjusted $r^2$ | $1 - \frac{n-1}{n-p}\frac{SSE}{SSTO}$ | As the SSE always decreases with the additional of more independent variables, optimising for the highest multiple $r^2$ value in the trained model is misleading. The adjusted $r^2$ penalises increasing the number of independent variables in the trained model as $\frac{n-1}{n-p}$ increases when $p$ increases. This metric is a property of the trained model and should not be used to identify the best model. |    

To explain how the multiple and adjusted $r^2$ are derived, it is easiest to visualise the SSE, SSR and SSTO in a simple linear regression model.      

<Include new diagram>

We can also output the model metrics of our model into a tabular format using the `tidy()` function.   

```{r}
#| output: true

# Output trained model evaluation metrics as a tidy table ----------------------
# Sigma is equivalent to the residual standard error (RSE) printed by summary()
glance(mlr_model)
```


## Interpreting residual plots      


# Advantages of using linear regression  


# Disadvantages of using linear regression  


# Linear regression with `tidymodels`   

```{r}
# Build model
# Make a new prediction
# Note that the predicted value is technically a point estimate of E(Y) rather than Y


```


# Linear regression with `mlr3`   

```{r}
# Build model
# Make a new prediction
# Note that the predicted value is technically a point estimate of E(Y) rather than Y


```


# Other resources     

+ https://andrewproctor.github.io/rcourse/module5.html#regression_basics    
+ https://andrewproctor.github.io/rcourse/assets/module5.pdf   
+ https://www.andrew.cmu.edu/user/achoulde/94842/lectures/lecture09/lecture09-94842.html    
+ https://www.andrew.cmu.edu/user/achoulde/94842/lectures/lecture11/lecture11-94842-2020.html    